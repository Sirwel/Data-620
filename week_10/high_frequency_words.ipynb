{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib import request\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score,\n",
        "    accuracy_score, confusion_matrix\n",
        ")\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import display_html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The IMDB Dataset is a widely recognized benchmark in natural language processing, primarily used for document classification and sentiment analysis. It comprises a large collection of movie reviews from the Internet Movie Database (IMDB), each labeled as either positive or negative, providing a balanced and well-structured corpus for analyzing text-based sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each movie review was cleaned to remove noise and ensure consistency. The text was converted to lowercase, HTML tags and punctuation were removed, and extra spaces were collapsed.  \n",
        "The cleaned text was stored in a new column called **clean_review**. Sentiment labels were also converted from **“positive”** and **“negative”** to binary values (**1** and **0**, respectively) to prepare the data for machine learning classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "movies_df = pd.read_csv(\"IMDB Dataset.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)  \n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)    \n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
        "    return text\n",
        "\n",
        "movies_df[\"clean_review\"] = movies_df[\"review\"].apply(clean_text)\n",
        "movies_df[\"label\"] = movies_df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_6835a\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_6835a_level0_col0\" class=\"col_heading level0 col0\" >clean_review</th>\n",
              "      <th id=\"T_6835a_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_6835a_row0_col0\" class=\"data row0 col0\" >one of the other reviewers has mentioned that after watching just oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn t mess around the first episode i ever saw struck me as so nasty it was surreal i couldn t say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side</td>\n",
              "      <td id=\"T_6835a_row0_col1\" class=\"data row0 col1\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6835a_row1_col0\" class=\"data row1 col0\" >a wonderful little production the filming technique is very unassuming very old time bbc fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great master s of comedy and his life the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwell s murals decorating every surface are terribly well done</td>\n",
              "      <td id=\"T_6835a_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6835a_row2_col0\" class=\"data row2 col0\" >i thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air conditioned theater and watching a light hearted comedy the plot is simplistic but the dialogue is witty and the characters are likable even the well bread suspected serial killer while some may be disappointed when they realize this is not match point risk addiction i thought it was proof that woody allen is still fully in control of the style many of us have grown to love this was the most i d laughed at one of woody s comedies in years dare i say a decade while i ve never been impressed with scarlet johanson in this she managed to tone down her sexy image and jumped right into a average but spirited young woman this may not be the crown jewel of his career but it was wittier than devil wears prada and more interesting than superman a great comedy to go see with friends</td>\n",
              "      <td id=\"T_6835a_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6835a_row3_col0\" class=\"data row3 col0\" >basically there s a family where a little boy jake thinks there s a zombie in his closet his parents are fighting all the time this movie is slower than a soap opera and suddenly jake decides to become rambo and kill the zombie ok first of all when you re going to make a film you must decide if its a thriller or a drama as a drama the movie is watchable parents are divorcing arguing like in real life and then we have jake with his closet which totally ruins all the film i expected to see a boogeyman similar movie and instead i watched a drama with some meaningless thriller spots out of just for the well playing parents descent dialogs as for the shots with jake just ignore them</td>\n",
              "      <td id=\"T_6835a_row3_col1\" class=\"data row3 col1\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6835a_row4_col0\" class=\"data row4 col0\" >petter mattei s love in the time of money is a visually stunning film to watch mr mattei offers us a vivid portrait about human relations this is a movie that seems to be telling us what money power and success do to people in the different situations we encounter this being a variation on the arthur schnitzler s play about the same theme the director transfers the action to the present time new york where all these different characters meet and connect each one is connected in one way or another to the next person but no one seems to know the previous point of contact stylishly the film has a sophisticated luxurious look we are taken to see how these people live and the world they live in their own habitat the only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits a big city is not exactly the best place in which human relations find sincere fulfillment as one discerns is the case with most of the people we encounter the acting is good under mr mattei s direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier and the rest of the talented cast make these characters come alive we wish mr mattei good luck and await anxiously for his next work</td>\n",
              "      <td id=\"T_6835a_row4_col1\" class=\"data row4 col1\" >1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x753dea27fbf0>"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_df[[\"clean_review\",\"label\"]].head().style.hide(axis=\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset was divided into training and testing subsets using an 80/20 split. The **random_state 456** ensures the split is reproducible, and **stratify=movies_df[\"label\"]** maintains the same proportion of positive and negative reviews in both sets, preserving class balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train,x_test, y_train,y_test = train_test_split(\n",
        "    movies_df[\"clean_review\"],\n",
        "    movies_df[\"label\"],\n",
        "    train_size=0.8,\n",
        "    test_size=0.2, \n",
        "    random_state=456,\n",
        "    stratify=movies_df[\"label\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To transform the textual movie reviews into a numerical format suitable for machine learning, **Word2Vec** was used as the feature extraction method. This approach converts each word into a dense vector that captures both syntactic and semantic relationships, allowing the model to understand how words relate to one another in context.\n",
        "\n",
        "The reviews were first tokenized into individual words and then used to train a Word2Vec model with key parameters chosen to enhance representation quality. The **vector_size** was set to **500**, meaning each word is represented as a 500-dimensional vector, providing enough capacity to encode complex linguistic features. The **window** parameter was set to **6**, defining the number of neighboring words on each side that the model considers when learning word associations. The **min_count** was set to **2**, filtering out words that appear only once and thus reducing noise from rare or misspelled terms.\n",
        "\n",
        "Training was parallelized with **workers=4** for faster computation, and **sg=1** was used to apply the **skip-gram** architecture, which is particularly effective at learning high-quality embeddings for less frequent words by predicting context words from a target word.\n",
        "\n",
        "Once trained, each review was transformed into a single vector using the `document_vector()` function, which computes the average of all word vectors in the review. This process produced consistent-length feature representations (**v_train_set** and **v_test_set**) that capture the overall meaning of each review and serve as input features for the classification models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vectorizer = TfidfVectorizer(max_features=5000,stop_words=\"english\",ngram_range=(1,4))\n",
        "# v_train_set = vectorizer.fit_transform(x_train)\n",
        "# v_test_set =  vectorizer.fit_transform(x_test)\n",
        "x_train_tokens = [text.split() for text in x_train]\n",
        "x_test_tokens  = [text.split() for text in x_test]\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=x_train_tokens,\n",
        "    vector_size=500,\n",
        "    window=6,\n",
        "    min_count=2,\n",
        "    workers=4,\n",
        "    sg=1\n",
        ")\n",
        "\n",
        "def document_vector(words):\n",
        "    words = [w for w in words if w in w2v_model.wv]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    return np.mean(w2v_model.wv[words], axis=0)\n",
        "\n",
        "v_train_set = np.vstack([document_vector(words) for words in x_train_tokens])\n",
        "v_test_set  = np.vstack([document_vector(words) for words in x_test_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w2v_model.save(\"w2v_model_imdb.model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Four models were trained to classify IMDB movie reviews: **Support Vector Machine (SVM)**, **Logistic Regression**, **Random Forest**, and **XGBoost**. Each model aimed to predict whether a review expressed a positive or negative sentiment using the cleaned text data.\n",
        "\n",
        "The models were evaluated using six metrics: **Accuracy**, **Precision**, **Recall**, **Sensitivity**, **Specificity**, and **F1-score**. These metrics provided a balanced view of overall and class-specific performance, helping identify which model achieved the best results while minimizing misclassifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_param = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_metrics = [\n",
        "        \"Set\",\n",
        "        \"Accuracy\",\n",
        "        \"Precision\",\n",
        "        \"Recall\",\n",
        "        \"Sensitivity\",\n",
        "        \"Specificity\",\n",
        "        \"F1\"\n",
        "        ]\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "   \n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    TP, FN, FP, TN = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) else 0\n",
        "    specificity = TN / (TN + FP) if (TN + FP) else 0\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"Sensitivity\": sensitivity,\n",
        "        \"Specificity\": specificity,\n",
        "        \"F1\": f1\n",
        "    }\n",
        "    \n",
        "def generate_report(model_instance,trainX,trainY,testX,testY):\n",
        "    y_train_pred = model_instance.predict(trainX)\n",
        "    y_test_pred = model_instance.predict(testX)\n",
        "    train_set_metrics = evaluate_model(trainY,y_train_pred)\n",
        "    test_set_metrics = evaluate_model(testY,y_test_pred)\n",
        "    train_set_metrics[\"Set\"] = \"Training\"\n",
        "    test_set_metrics[\"Set\"] = \"Test\"\n",
        "    model_metrics_df = pd.DataFrame(columns=model_metrics,data= [train_set_metrics,test_set_metrics])\n",
        "    styled_report = model_metrics_df.style.hide(axis=\"index\")\n",
        "    return model_metrics_df,styled_report\n",
        "\n",
        "def display_side_by_side(dfs, titles=None):\n",
        "    html_str = \"<div style='display:flex;flex-flow:row nowrap;column-gap:20px'>\"\n",
        "    for df, title in zip(dfs, titles):\n",
        "        html_str += f\"\"\"\n",
        "        <div style=\"margin:10px\">\n",
        "            <h4 style=\"text-align:center\">{title}</h4>\n",
        "            {df.to_html()}\n",
        "        </div>\"\"\"\n",
        "    html_str += \"</div>\"\n",
        "\n",
        "    display_html(html_str, raw=True)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Support Vector Machine model was tuned using a Grid Search focused on the regularization parameter **C**, tested from **0.001 to 1** in increments of **0.009**. This parameter controls the trade-off between fitting the training data well and maintaining good generalization on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_param_grid = {'C': np.arange(0.001, 1, 0.009)}\n",
        "svm_model = LinearSVC(random_state=500)\n",
        "\n",
        "grid = GridSearchCV(svm_model, svm_param_grid, cv=cv_param, scoring='accuracy', n_jobs=-1, verbose=0)\n",
        "grid.fit(v_train_set, y_train)\n",
        "# print(\"Best Parameters:\", grid.best_params_)\n",
        "# print(\"Best CV Accuracy:\", round(grid.best_score_, 3))\n",
        "svm_model = grid.best_estimator_\n",
        "\n",
        "svm_df, svm_df_styled = generate_report(svm_model,v_train_set,y_train,v_test_set,y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Logistic Regression, a Grid Search explored **C** values from **0.001 to 1** in increments of **0.005**, along with two solver options: **liblinear** and **lbfgs**. These settings helped identify the best regularization strength and optimization approach for handling text-based sentiment data efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "log_param_grid = {'C': np.arange(0.001, 1, 0.005), 'solver': ['liblinear', 'lbfgs']}\n",
        "\n",
        "grid = GridSearchCV(estimator=log_reg, param_grid=log_param_grid,cv=cv_param, scoring='accuracy', n_jobs=-1, verbose=0)\n",
        "\n",
        "grid.fit(v_train_set, y_train)\n",
        "\n",
        "# print(\"Best Parameters:\", grid.best_params_)\n",
        "# print(\"Best CV Accuracy:\", round(grid.best_score_, 3))\n",
        "\n",
        "logistic_model = grid.best_estimator_\n",
        "\n",
        "lgreg_df, lgreg_df_styled = generate_report(logistic_model,v_train_set,y_train,v_test_set,y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The XGBoost model was trained using a parameter grid that focused on optimizing learning speed and predictive performance. The **n_estimators** parameter was tested with **100** and **500** trees to evaluate the impact of ensemble size on accuracy and overfitting. The **learning_rate** values (**0.01**, **0.1**, and **0.2**) controlled how much the model adjusted with each boosting step, balancing convergence speed and model stability.\n",
        "\n",
        "Although the **max_depth** parameter was considered during experimentation, it was later omitted to simplify the tuning process and reduce overfitting risk. These parameter settings allowed the model to capture nonlinear relationships efficiently while maintaining good generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "xgboost = GradientBoostingClassifier(random_state=500)\n",
        "xgboost_param_grid = {'n_estimators':[100,500],'learning_rate':[0.01,0.1,0.2]\n",
        "                    #   ,'max_depth':[2,3,4]\n",
        "                      }\n",
        "grid = GridSearchCV(xgboost,param_grid=xgboost_param_grid,cv=cv_param,scoring='accuracy',n_jobs=-1,verbose=0)\n",
        "grid.fit(v_train_set,y_train)\n",
        "# print(\"Best Params:\",grid.best_params_)\n",
        "# print(\"Best CV Accuracy:\",round(grid.best_score_,3))\n",
        "xgboost = grid.best_estimator_\n",
        "\n",
        "naxgboost_df, xgboost_df_styled = generate_report(xgboost,v_train_set,y_train,v_test_set,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Random Forest model was trained using a parameter grid that aimed to balance accuracy and generalization. The **n_estimators** values (**200** and **400**) controlled the number of trees, while **max_depth** (**None** and **20**) adjusted tree growth to manage complexity. The **max_features** parameter was set to **\"sqrt\"** to reduce feature correlation and improve model diversity.\n",
        "\n",
        "The grid also tuned **min_samples_split** (**2**, **5**) and **min_samples_leaf** (**1**, **2**) to control how many samples were needed to create or end splits in each tree. These settings helped identify the best configuration for stable and efficient model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_dd4fb\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_dd4fb_level0_col0\" class=\"col_heading level0 col0\" >Set</th>\n",
              "      <th id=\"T_dd4fb_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_dd4fb_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
              "      <th id=\"T_dd4fb_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
              "      <th id=\"T_dd4fb_level0_col4\" class=\"col_heading level0 col4\" >Sensitivity</th>\n",
              "      <th id=\"T_dd4fb_level0_col5\" class=\"col_heading level0 col5\" >Specificity</th>\n",
              "      <th id=\"T_dd4fb_level0_col6\" class=\"col_heading level0 col6\" >F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_dd4fb_row0_col0\" class=\"data row0 col0\" >Training</td>\n",
              "      <td id=\"T_dd4fb_row0_col1\" class=\"data row0 col1\" >0.999025</td>\n",
              "      <td id=\"T_dd4fb_row0_col2\" class=\"data row0 col2\" >0.999800</td>\n",
              "      <td id=\"T_dd4fb_row0_col3\" class=\"data row0 col3\" >0.998250</td>\n",
              "      <td id=\"T_dd4fb_row0_col4\" class=\"data row0 col4\" >0.999800</td>\n",
              "      <td id=\"T_dd4fb_row0_col5\" class=\"data row0 col5\" >0.998250</td>\n",
              "      <td id=\"T_dd4fb_row0_col6\" class=\"data row0 col6\" >0.999024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_dd4fb_row1_col0\" class=\"data row1 col0\" >Test</td>\n",
              "      <td id=\"T_dd4fb_row1_col1\" class=\"data row1 col1\" >0.841400</td>\n",
              "      <td id=\"T_dd4fb_row1_col2\" class=\"data row1 col2\" >0.831456</td>\n",
              "      <td id=\"T_dd4fb_row1_col3\" class=\"data row1 col3\" >0.856400</td>\n",
              "      <td id=\"T_dd4fb_row1_col4\" class=\"data row1 col4\" >0.826400</td>\n",
              "      <td id=\"T_dd4fb_row1_col5\" class=\"data row1 col5\" >0.856400</td>\n",
              "      <td id=\"T_dd4fb_row1_col6\" class=\"data row1 col6\" >0.843744</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x753e1ba21e80>"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_forest = RandomForestClassifier(random_state=500)\n",
        "\n",
        "rf_param_grid = param_grid = {\n",
        "    \"n_estimators\": [200, 400],\n",
        "    \"max_depth\": [None, 20],\n",
        "    \"max_features\": [\"sqrt\"],\n",
        "    \"min_samples_split\": [2, 5],\n",
        "    \"min_samples_leaf\": [1, 2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=random_forest, param_grid=rf_param_grid,cv=cv_param, scoring='accuracy', n_jobs=-1, verbose=0)\n",
        "\n",
        "grid.fit(v_train_set, y_train)\n",
        "\n",
        "# print(\"Best Parameters:\", grid.best_params_)\n",
        "# print(\"Best CV Accuracy:\", round(grid.best_score_, 3))\n",
        "\n",
        "random_forest = grid.best_estimator_\n",
        "\n",
        "random_forest_df, random_forest_df_styled = generate_report(random_forest,v_train_set,y_train,v_test_set,y_test)\n",
        "random_forest_df_styled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style='display:flex;flex-flow:row nowrap;column-gap:20px'>\n",
              "        <div style=\"margin:10px\">\n",
              "            <h4 style=\"text-align:center\">Training Evaluation</h4>\n",
              "            <style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_cc0a9\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_cc0a9_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_cc0a9_level0_col1\" class=\"col_heading level0 col1\" >Set</th>\n",
              "      <th id=\"T_cc0a9_level0_col2\" class=\"col_heading level0 col2\" >Accuracy</th>\n",
              "      <th id=\"T_cc0a9_level0_col3\" class=\"col_heading level0 col3\" >Precision</th>\n",
              "      <th id=\"T_cc0a9_level0_col4\" class=\"col_heading level0 col4\" >Recall</th>\n",
              "      <th id=\"T_cc0a9_level0_col5\" class=\"col_heading level0 col5\" >Sensitivity</th>\n",
              "      <th id=\"T_cc0a9_level0_col6\" class=\"col_heading level0 col6\" >Specificity</th>\n",
              "      <th id=\"T_cc0a9_level0_col7\" class=\"col_heading level0 col7\" >F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_cc0a9_row0_col0\" class=\"data row0 col0\" >SVM</td>\n",
              "      <td id=\"T_cc0a9_row0_col1\" class=\"data row0 col1\" >Training</td>\n",
              "      <td id=\"T_cc0a9_row0_col2\" class=\"data row0 col2\" >0.890675</td>\n",
              "      <td id=\"T_cc0a9_row0_col3\" class=\"data row0 col3\" >0.888171</td>\n",
              "      <td id=\"T_cc0a9_row0_col4\" class=\"data row0 col4\" >0.893900</td>\n",
              "      <td id=\"T_cc0a9_row0_col5\" class=\"data row0 col5\" >0.887450</td>\n",
              "      <td id=\"T_cc0a9_row0_col6\" class=\"data row0 col6\" >0.893900</td>\n",
              "      <td id=\"T_cc0a9_row0_col7\" class=\"data row0 col7\" >0.891026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_cc0a9_row1_col0\" class=\"data row1 col0\" >Random Forest</td>\n",
              "      <td id=\"T_cc0a9_row1_col1\" class=\"data row1 col1\" >Training</td>\n",
              "      <td id=\"T_cc0a9_row1_col2\" class=\"data row1 col2\" >0.999025</td>\n",
              "      <td id=\"T_cc0a9_row1_col3\" class=\"data row1 col3\" >0.999800</td>\n",
              "      <td id=\"T_cc0a9_row1_col4\" class=\"data row1 col4\" >0.998250</td>\n",
              "      <td id=\"T_cc0a9_row1_col5\" class=\"data row1 col5\" >0.999800</td>\n",
              "      <td id=\"T_cc0a9_row1_col6\" class=\"data row1 col6\" >0.998250</td>\n",
              "      <td id=\"T_cc0a9_row1_col7\" class=\"data row1 col7\" >0.999024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_cc0a9_row2_col0\" class=\"data row2 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_cc0a9_row2_col1\" class=\"data row2 col1\" >Training</td>\n",
              "      <td id=\"T_cc0a9_row2_col2\" class=\"data row2 col2\" >0.882925</td>\n",
              "      <td id=\"T_cc0a9_row2_col3\" class=\"data row2 col3\" >0.880811</td>\n",
              "      <td id=\"T_cc0a9_row2_col4\" class=\"data row2 col4\" >0.885700</td>\n",
              "      <td id=\"T_cc0a9_row2_col5\" class=\"data row2 col5\" >0.880150</td>\n",
              "      <td id=\"T_cc0a9_row2_col6\" class=\"data row2 col6\" >0.885700</td>\n",
              "      <td id=\"T_cc0a9_row2_col7\" class=\"data row2 col7\" >0.883249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_cc0a9_row3_col0\" class=\"data row3 col0\" >XGBoost</td>\n",
              "      <td id=\"T_cc0a9_row3_col1\" class=\"data row3 col1\" >Training</td>\n",
              "      <td id=\"T_cc0a9_row3_col2\" class=\"data row3 col2\" >0.927750</td>\n",
              "      <td id=\"T_cc0a9_row3_col3\" class=\"data row3 col3\" >0.925326</td>\n",
              "      <td id=\"T_cc0a9_row3_col4\" class=\"data row3 col4\" >0.930600</td>\n",
              "      <td id=\"T_cc0a9_row3_col5\" class=\"data row3 col5\" >0.924900</td>\n",
              "      <td id=\"T_cc0a9_row3_col6\" class=\"data row3 col6\" >0.930600</td>\n",
              "      <td id=\"T_cc0a9_row3_col7\" class=\"data row3 col7\" >0.927955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "\n",
              "        </div>\n",
              "        <div style=\"margin:10px\">\n",
              "            <h4 style=\"text-align:center\">Test Set Evaluation</h4>\n",
              "            <style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_28e07\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_28e07_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_28e07_level0_col1\" class=\"col_heading level0 col1\" >Set</th>\n",
              "      <th id=\"T_28e07_level0_col2\" class=\"col_heading level0 col2\" >Accuracy</th>\n",
              "      <th id=\"T_28e07_level0_col3\" class=\"col_heading level0 col3\" >Precision</th>\n",
              "      <th id=\"T_28e07_level0_col4\" class=\"col_heading level0 col4\" >Recall</th>\n",
              "      <th id=\"T_28e07_level0_col5\" class=\"col_heading level0 col5\" >Sensitivity</th>\n",
              "      <th id=\"T_28e07_level0_col6\" class=\"col_heading level0 col6\" >Specificity</th>\n",
              "      <th id=\"T_28e07_level0_col7\" class=\"col_heading level0 col7\" >F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_28e07_row0_col0\" class=\"data row0 col0\" >SVM</td>\n",
              "      <td id=\"T_28e07_row0_col1\" class=\"data row0 col1\" >Test</td>\n",
              "      <td id=\"T_28e07_row0_col2\" class=\"data row0 col2\" >0.888000</td>\n",
              "      <td id=\"T_28e07_row0_col3\" class=\"data row0 col3\" >0.882794</td>\n",
              "      <td id=\"T_28e07_row0_col4\" class=\"data row0 col4\" >0.894800</td>\n",
              "      <td id=\"T_28e07_row0_col5\" class=\"data row0 col5\" >0.881200</td>\n",
              "      <td id=\"T_28e07_row0_col6\" class=\"data row0 col6\" >0.894800</td>\n",
              "      <td id=\"T_28e07_row0_col7\" class=\"data row0 col7\" >0.888756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_28e07_row1_col0\" class=\"data row1 col0\" >Random Forest</td>\n",
              "      <td id=\"T_28e07_row1_col1\" class=\"data row1 col1\" >Test</td>\n",
              "      <td id=\"T_28e07_row1_col2\" class=\"data row1 col2\" >0.841400</td>\n",
              "      <td id=\"T_28e07_row1_col3\" class=\"data row1 col3\" >0.831456</td>\n",
              "      <td id=\"T_28e07_row1_col4\" class=\"data row1 col4\" >0.856400</td>\n",
              "      <td id=\"T_28e07_row1_col5\" class=\"data row1 col5\" >0.826400</td>\n",
              "      <td id=\"T_28e07_row1_col6\" class=\"data row1 col6\" >0.856400</td>\n",
              "      <td id=\"T_28e07_row1_col7\" class=\"data row1 col7\" >0.843744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_28e07_row2_col0\" class=\"data row2 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_28e07_row2_col1\" class=\"data row2 col1\" >Test</td>\n",
              "      <td id=\"T_28e07_row2_col2\" class=\"data row2 col2\" >0.879900</td>\n",
              "      <td id=\"T_28e07_row2_col3\" class=\"data row2 col3\" >0.875321</td>\n",
              "      <td id=\"T_28e07_row2_col4\" class=\"data row2 col4\" >0.886000</td>\n",
              "      <td id=\"T_28e07_row2_col5\" class=\"data row2 col5\" >0.873800</td>\n",
              "      <td id=\"T_28e07_row2_col6\" class=\"data row2 col6\" >0.886000</td>\n",
              "      <td id=\"T_28e07_row2_col7\" class=\"data row2 col7\" >0.880628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_28e07_row3_col0\" class=\"data row3 col0\" >XGBoost</td>\n",
              "      <td id=\"T_28e07_row3_col1\" class=\"data row3 col1\" >Test</td>\n",
              "      <td id=\"T_28e07_row3_col2\" class=\"data row3 col2\" >0.878800</td>\n",
              "      <td id=\"T_28e07_row3_col3\" class=\"data row3 col3\" >0.875050</td>\n",
              "      <td id=\"T_28e07_row3_col4\" class=\"data row3 col4\" >0.883800</td>\n",
              "      <td id=\"T_28e07_row3_col5\" class=\"data row3 col5\" >0.873800</td>\n",
              "      <td id=\"T_28e07_row3_col6\" class=\"data row3 col6\" >0.883800</td>\n",
              "      <td id=\"T_28e07_row3_col7\" class=\"data row3 col7\" >0.879403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "\n",
              "        </div></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "svm_df[\"Model\"] = \"SVM\"\n",
        "random_forest_df[\"Model\"] = \"Random Forest\"\n",
        "lgreg_df[\"Model\"] = \"Logistic Regression\"\n",
        "naxgboost_df[\"Model\"] = \"XGBoost\"\n",
        "\n",
        "combined_df = pd.concat([svm_df, random_forest_df, lgreg_df, naxgboost_df], ignore_index=True)\n",
        "cols = [\"Model\"] + [col for col in combined_df.columns if col != \"Model\"]\n",
        "combined_df = combined_df[cols]\n",
        "training_summary = combined_df[combined_df[\"Set\"] == \"Training\"].style.hide(axis=\"index\")\n",
        "test_summary = combined_df[combined_df[\"Set\"] == \"Test\"].style.hide(axis=\"index\")\n",
        "\n",
        "\n",
        "display_side_by_side([training_summary,test_summary],[\"Training Evaluation\",\"Test Set Evaluation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The training results indicate that all models performed strongly after incorporating Word2Vec embeddings, which effectively captured semantic relationships between words and improved text representation. The **Random Forest** model achieved the highest training accuracy (**0.999**), along with near-perfect precision (**0.9998**), recall (**0.9983**), and F1 (**0.9990**). However, this exceptional performance did not carry over to the test set, where accuracy dropped to **0.841** and the F1 score to **0.8437**. This sharp contrast suggests that the Random Forest model memorized patterns from the training data instead of learning generalizable relationships, a clear indication of overfitting.\n",
        "\n",
        "The **SVM**, **Logistic Regression**, and **XGBoost** models demonstrated more consistent behavior between training and test results. The **SVM** achieved a training accuracy of **0.8907** and a test accuracy of **0.888**, showing minimal performance drop. Similarly, **Logistic Regression** performed with **0.8829** accuracy in training and **0.8799** on the test set, while **XGBoost** achieved **0.9278** and **0.8788**, respectively. The closeness of these metrics across both sets reflects strong model generalization and the ability to classify sentiment effectively without overfitting.\n",
        "\n",
        "In terms of balance between sensitivity and specificity, the **SVM** and **XGBoost** models performed particularly well, maintaining consistent values across both sets. The **SVM** achieved the highest F1 score on the test data (**0.8888**), confirming its reliability in identifying both positive and negative sentiments with high precision. Logistic Regression followed closely, also maintaining a strong trade-off between recall and precision.\n",
        "\n",
        "Overall, the results confirm that the **Word2Vec embeddings contributed most to the performance gains** by providing dense and meaningful text features. The small variations in accuracy and F1 across models indicate that the embedding layer played the key role in enabling robust sentiment classification. While the **SVM** model slightly outperformed the others in generalization, **Logistic Regression** and **XGBoost** produced similarly strong and stable results, making any of the three a valid choice for deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This project demonstrated the effectiveness of combining **Word2Vec embeddings** with traditional machine learning algorithms for sentiment classification on the IMDB movie review dataset. Through systematic data cleaning, feature extraction, and model evaluation, it became clear that the quality of text representation played the most significant role in achieving strong model performance. The Word2Vec model captured meaningful semantic relationships between words, allowing classifiers to interpret sentiment and context more effectively.\n",
        "\n",
        "Among the four models tested, **SVM**, **Logistic Regression**, **Random Forest**, and **XGBoost**, the **SVM** achieved the best balance between accuracy and generalization, while Logistic Regression and XGBoost produced similar, consistent results. The Random Forest model achieved near-perfect accuracy during training but showed clear signs of overfitting when evaluated on unseen data.\n",
        "\n",
        "Overall, the findings highlight that **feature quality is more important than model complexity** in text classification tasks. The Word2Vec embeddings provided a solid foundation for sentiment understanding, enabling even simple models to perform well. Future work could involve fine-tuning embedding parameters, using pre-trained Word2Vec or Transformer-based models, and extending this framework to more diverse or multi-class datasets to further improve generalization and interpretability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
